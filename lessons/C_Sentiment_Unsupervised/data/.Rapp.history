setwd("/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data/z_rap_songs")
getwd()
setwd('/Desktop')
# Set wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data/z_rap_songs")#
# Options#
options(stringsAsFactors = F, scipen = 999)#
#
# libs#
library(stringr)#
library(ggplot2)#
library(ggthemes)#
library(pbapply)#
#
# Multiple files as a list#
tmp <- list.files(pattern = '*.csv')#
allSongs <- pblapply(tmp, read.csv)#
names(allSongs) <- gsub('csv','', tmp)#
#
# Basic Exploration#
allSongs$BEST.ON.EARTH..Bonus...Explicit..by.Russ.feat..BIA.#
#
## Length of each song#
songLength <- sapply(allSongs, function(x){ max(x[,1])}) #
songLength <- round((songLength /1000)/60, 2)#
#
## Avg words in song#
singleWords <- list()#
for(i in 1:length(allSongs)){#
  print(names(allSongs)[i])#
  x <- strsplit(as.character(allSongs[[i]][,3])," ")#
  singleWords[[i]] <- data.frame(song = names(allSongs)[i],#
                                 totalWords  = length(unlist(x)))#
}#
singleWords <- do.call(rbind, singleWords)#
head(singleWords)
sapply(allSongs, function(x) grep('trippin', x[,3], ignore.case = T))
sapply(allSongs, function(x) grep('money', x[,3], ignore.case = T))
sapply(allSongs, function(x) grepl('money', x[,3], ignore.case = T))
# Or find them at the song level#
searchTerm <- 'money'#
termExist <- list()#
for(i in 1:length(allSongs)){#
  x <- paste(allSongs[[i]][,3], collapse = ' ')#
  x <- grepl(searchTerm, x, ignore.case = T)#
  termDF <- data.frame(song  = names(allSongs[i]),#
                       exist = x)#
  names(termDF)[2] <- paste0(searchTerm, '_exists')#
  termExist[[i]] <- termDF#
}#
termExist <- do.call(rbind, termExist)
## stricount words#
countWords <- function(docDF, termVector){#
  response <- list()#
  for(i in 1:length(termVector)){#
    x <- tolower(docDF[,3])#
    x <- sum(str_count(x, termVector[i]))#
    response[[i]] <- x #
  }
response <- do.call(cbind, response)#
  colnames(response) <- termVector#
  return(response)#
}
# Apply to one song as example#
countWords(allSongs[[1]],c('trippin', 'money'))
# Apply to list#
wordCheck <- lapply(allSongs, countWords, c('trippin', 'money'))#
wordCheck <- data.frame(song = names(wordCheck),#
                        do.call(rbind, wordCheck))#
wordCheck
# Calculate the cumulative sum#
wordCountList <- list()#
for(i in 1:length(allSongs)){#
  x <- allSongs[[i]]#
  wordCount <- str_count(x$text, "\\S+") #count the space character#
  y <- data.frame(x$endTime, #
                  cumulativeWords = cumsum(wordCount),#
                  song = names(allSongs[i]),#
                  lyric = x$text)#
  names(y)[1] <- 'endTime'#
  wordCountList[[i]] <- y#
}
# Get the timeline of a song#
songTimeline  <- do.call(rbind, wordCountList)#
head(songTimeline)
# Get the last values for each song (total words but now with time)#
totalWords <- lapply(wordCountList, tail,1)#
totalWords <- do.call(rbind, totalWords)#
#
# Make a plot of the speech cadence#
ggplot(songTimeline,  aes(x     = endTime,#
                          y     = cumulativeWords, #
                          group = song, #
                          color = song)) +#
  geom_line(alpha = 0.25) +#
  geom_point(data =totalWords, aes(x     = endTime,#
                                   y     = cumulativeWords, #
                                   group = song, #
                                   color = song), size = 2) +#
  geom_text(data  = totalWords, aes(label=song),#
            hjust = "inward", vjust = "inward", size = 3) + #
  theme_tufte() + theme(legend.position = "none")
# Two clusters, let's see Em vs all#
songTimeline$eminem <- grepl('eminem', #
                             songTimeline$song, #
                             ignore.case = T)#
totalWords$eminem <- grepl('eminem', #
                           totalWords$song, #
                           ignore.case = T)#
ggplot(songTimeline,  aes(x     = endTime,#
                          y     = cumulativeWords, #
                          group = song, #
                          color = eminem)) +#
  geom_line(alpha = 0.25) +#
  geom_point(data =totalWords, aes(x     = endTime,#
                                   y     = cumulativeWords, #
                                   group = song, #
                                   color = eminem), size = 2) +#
  geom_text(data  = totalWords, aes(label=song),#
            hjust = "inward", vjust = "inward", size = 3) + #
  theme_few() + theme(legend.position = "none")
# Fit a linear model to each song and extract the x-coefficient#
# Poached: https://stackoverflow.com/questions/40284801/how-to-calculate-the-slopes-of-different-linear-regression-lines-on-multiple-plo#
library(tidyr)#
library(purrr)#
library(dplyr)#
doModel  <- function(dat) {lm(cumulativeWords ~ endTime + 0, dat)}#
getSlope <- function(mod) {coef(mod)[2]}#
models <- songTimeline %>% #
  group_by(song) %>%#
  nest %>% #tidyr::Nest Repeated Values In A List-Variable.#
  mutate(model = map(data, doModel)) %>% #
  mutate(slope = map(model, coefficients)) #
#
# Avg words per second by song#
wordsSecs <- data.frame(song = names(allSongs),#
                        wordsPerSecond= (unlist(models$slope) * 1000)) #adj for milliseconds#
wordsSecs[order(wordsSecs$wordsPerSecond, decreasing = T),]
getwd()
# Set the working directory#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data/z_rap_songs")#
#
# Libs#
library(tm)#
library(qdap)#
library(plotrix)#
library(ggplot2)#
library(ggthemes)#
library(ggalt)
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
# Create custom stop words#
stops <- c(stopwords('SMART'), 'amp', 'britishairways', #
           'british', 'flight', 'flights', 'airways', #
           'ryanair', 'airline', 'flying')
# Read in Data, clean & organize.  Wrapped in another function for you!#
# No qdap? Go to the directly to ZZZ Supporting" & remove  contraction in clean corpus#
textA <- cleanMatrix(pth             = 'BritishAirways.csv',#
                     columnName      = 'text',#
                     collapse        = T, #
                     customStopwords = stops,#
                     type = 'TDM', # TDM or DTM#
                     wgt = 'weightTf')
list.files()
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")
# Read in Data, clean & organize.  Wrapped in another function for you!#
# No qdap? Go to the directly to ZZZ Supporting" & remove  contraction in clean corpus#
textA <- cleanMatrix(pth             = 'BritishAirways.csv',#
                     columnName      = 'text',#
                     collapse        = T, #
                     customStopwords = stops,#
                     type = 'TDM', # TDM or DTM#
                     wgt = 'weightTf')
textB <- cleanMatrix(pth        = 'RyanAir.csv',#
                     columnName = 'text',#
                     collapse   = T,#
                     customStopwords = stops,#
                     type = 'TDM', # TDM or DTM#
                     wgt = 'weightTf')
df        <- merge(textA, textB, by ='row.names')#
names(df) <- c('terms', 'britishAir', 'ryanAir')
# Examine#
df[6:10,]
df$diff <- abs(df$britishAir - df$ryanAir)
pyramid.plot(lx         = top35$britishAir, #left#
             rx         = top35$ryanAir,    #right#
             labels     = top35$terms,  #terms#
             top.labels = c('britishAir', 'Terms', 'ryanAir'), #corpora#
             gap        = 5, # space for terms to be read#
             main       = 'Words in Common', # title#
             unit       = 'wordFreq')
# Organize df for plotting#
df<- df[order(df$diff, decreasing=TRUE), ]#
top35 <- df[1:35, ]#
#
# Pyarmid Plot#
pyramid.plot(lx         = top35$britishAir, #left#
             rx         = top35$ryanAir,    #right#
             labels     = top35$terms,  #terms#
             top.labels = c('britishAir', 'Terms', 'ryanAir'), #corpora#
             gap        = 5, # space for terms to be read#
             main       = 'Words in Common', # title#
             unit       = 'wordFreq')
getwd()
# libs#
library(ggplot2)#
library(tm)#
library(dplyr)#
#
# Set wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")#
#
# Options & Functions#
options(stringsAsFactors = FALSE, scipen = 999)#
Sys.setlocale('LC_ALL','C')#
#
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}#
#
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url))#
  #corpus <- tm_map(corpus, content_transformer(replace_contraction)) #
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}#
#
# Create custom stop words#
stops <- c(stopwords('english'), 'carrefour', 'tesco')#
#
# Bring in the two corpora; works with no meta data#
retailers <- Corpus(DirSource("polarizedCloud/"))#
#
# Get word counts#
# mind the order is the same as is list.files('~/Documents/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data/polarizedCloud')#
carreCount <- length(unlist(strsplit(content(retailers[[1]]), " ")))#
tescoCount <- length(unlist(strsplit(content(retailers[[2]]), " ")))#
#
# Clean & TDM#
cleanRetail <- cleanCorpus(retailers, stops)#
cleanRetail <- TermDocumentMatrix(cleanRetail)#
#
# Create data frame from TDM#
retailDF <- as.data.frame(as.matrix(cleanRetail))#
head(retailDF)
# subset and calc diff#
retailDF      <- subset(retailDF, retailDF[,1]>0 & retailDF[,2]>0) #in case stops make empty docs#
retailDF$diff <- retailDF[,1]-retailDF[,2]#
#
# Words used more by Carrefour#
carrefourDF <- subset(retailDF, diff > 0) # Said more  by carre#
# Words used more by Tesco#
tescoDF     <- subset(retailDF, diff < 0) # Said more by tesco#
#
# Calc how the much the term contributes to the specific corpus #
carrefourDF$density <- carrefourDF$carrefour.csv/carreCount#
tescoDF$density     <- tescoDF$carrefour.csv/tescoCount#
#
### Step 3: Create visualization#
topNum <- 20#
obama.df <- head(carrefourDF[order(carrefourDF$diff, decreasing = T),],topNum)#
palin.df <- head(tescoDF[order(abs(tescoDF$diff), decreasing = T),],topNum)#
#
ggplot(obama.df, aes(x=diff, y=density))+#
  geom_text(aes(#size=obama.df[,1], #
                label=row.names(obama.df), colour=diff),#
            hjust = "inward", vjust = "inward")+#
  geom_text(data=palin.df, #
            aes(x=diff, y=density, label=row.names(palin.df), #
                #size=palin.df[,1],#
                color=diff),#
            hjust = "inward", vjust = "inward")+#
  scale_size(range=c(3,11), name="Word Frequency")+scale_colour_gradient(low="darkred", high="darkblue", guide="none")+#
  scale_x_continuous(breaks=c(min(palin.df$diff),0,max(obama.df$diff)),labels=c("Said More about Tesco","Said Equally","Said More about Carrefour"))+#
  scale_y_continuous(breaks=c(0),labels=c(""))+xlab("")+ylab("")+theme_bw()#+
# Libs#
library(qdap)#
#
# Neutral#
polarity('neutral neutral neutral')#
(0 + 0 + 0)/sqrt(3)#
#
# Amplifier#
polarity('neutral very good')#
(0 + 0.8 + 1)/sqrt(3)#
#
# De-Amplifier#
polarity('neutral barely good')#
(-0.8 + 1) /sqrt(3)#
#
# Negation#
polarity('neutral not  good')#
(-1 *(0 + 1)) / sqrt(3)#
#
# Double Negation#
polarity('not not good')#
(-1*-1* (1)) / sqrt(3)#
#
# Order doesn't matter in the context cluster#
polarity('good not good')#
(-1*(1 + 1)) / sqrt(3)
getwd()
uals/data
setwd('~/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data')
###### Improvement for next time#
# Bring in another song lyric to compare in radar chart#
#######
#
# Wd#
setwd('~/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data')#
#
# Libs#
library(tm)#
library(lexicon)#
library(tidytext)#
library(dplyr)#
library(qdap)#
library(radarchart)#
#
# Bring in our supporting functions#
source('~/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')#
#
# Create custom stop words#
stops <- c(stopwords('english'))#
#
# Clean and Organize#
txtDTM <- cleanMatrix('Weeknd.csv',#
                      'text',#
                      collapse        = F, #
                      customStopwords = stops, #
                      type            = 'DTM', #
                      wgt             = 'weightTf')#
#
# Examine original & Compare
install.packages('lexicon')
###### Improvement for next time#
# Bring in another song lyric to compare in radar chart#
#######
#
# Wd#
setwd('~/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data')#
#
# Libs#
library(tm)#
library(lexicon)#
library(tidytext)#
library(dplyr)#
library(qdap)#
library(radarchart)#
#
# Bring in our supporting functions#
source('~/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')#
#
# Create custom stop words#
stops <- c(stopwords('english'))#
#
# Clean and Organize#
txtDTM <- cleanMatrix('Weeknd.csv',#
                      'text',#
                      collapse        = F, #
                      customStopwords = stops, #
                      type            = 'DTM', #
                      wgt             = 'weightTf')#
#
# Examine original & Compare
txtDTM[,1:10]#
dim(txtDTM)
tmp      <- as.DocumentTermMatrix(txtDTM, weighting = weightTf ) #
tidyCorp <- tidy(tmp)#
tidyCorp#
dim(tidyCorp)
bing <- get_sentiments(lexicon = c("bing"))
head(bing)
bingSent <- inner_join(tidyCorp, bing, by=c('term' = 'word'))#
bingSent
table(bingSent$sentiment, bingSent$count)
aggregate(count~sentiment,bingSent, sum)
polarity(read.csv('Weeknd.csv')$text)
afinn<-get_sentiments(lexicon = c("afinn"))
afinn<-get_sentiments(lexicon = c("afinn"))
head(afinn)
afinnSent <- inner_join(tidyCorp,afinn, by=c('term' = 'word'))#
afinnSent
# Quick Analysis#
weeknd <- read.csv('Weeknd.csv')$text#
weekndWords <- data.frame(word = unlist(strsplit(weeknd,' ')))#
weekndWords$word <- tolower(weekndWords$word )#
weekndWords <- left_join(weekndWords,afinn, by=c('word' = 'word'))#
weekndWords[is.na(weekndWords$value),2] <- 0#
plot(weekndWords$value, type="l", main="Quick Timeline of Identified Words")
nrc <- nrc_emotions#
head(nrc)
terms <- subset(nrc, rowSums(nrc[,2:9])!=0)#
sent  <- apply(terms[,2:ncol(terms)], 1, function(x)which(x>0))#
head(sent)
# Reshape#
nrcLex <- list()#
for(i in 1:length(sent)){#
  x <- sent[[i]]#
  x <- data.frame(term      = terms[i,1],#
                  sentiment = names(sent[[i]]))#
  nrcLex[[i]] <- x#
}
nrcLex <- do.call(rbind, nrcLex)#
head(nrcLex)
nrcSent <- inner_join(tidyCorp,nrcLex, by=c('term' = 'term'))#
nrcSent
table(nrcSent$sentiment)#
emos <- data.frame(table(nrcSent$sentiment))#
#emos <- emos[-c(6,7),] #drop columns if needed#
chartJSRadar(scores = emos, labelSize = 10, showLegend = F)
getwd()
# Wd#
setwd('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data')#
#
# Libs#
library(echarts4r)#
library(tm)#
library(qdap)#
library(pbapply)#
library(lda)#
library(LDAvis)#
library(dplyr)#
library(treemap)#
#
# Bring in our supporting functions#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')#
#
# In some cases, blank documents and words are created bc of preprocessing.  This will remove them.#
blankRemoval<-function(x){#
  x <- unlist(strsplit(x,' '))#
  x <- subset(x,nchar(x)>0)#
  x <- paste(x,collapse=' ')#
}#
#
# Each term is assigned to a topic, so this will tally for a document & assign the most frequent as membership#
docAssignment<-function(x){#
  x <- table(x)#
  x <- as.matrix(x)#
  x <- t(x)#
  x <-max.col(x)#
}#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
# Stopwords#
stops <- c(stopwords('SMART'), 'pakistan', 'gmt', 'pm')#
#
# Data articles from ~2016-04-04#
text <- readRDS("Guardian_text.rds")
install.packages('echarts4r')
# Wd#
setwd('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data')#
#
# Libs#
library(echarts4r)#
library(tm)#
library(qdap)#
library(pbapply)#
library(lda)#
library(LDAvis)#
library(dplyr)#
library(treemap)#
#
# Bring in our supporting functions#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')#
#
# In some cases, blank documents and words are created bc of preprocessing.  This will remove them.#
blankRemoval<-function(x){#
  x <- unlist(strsplit(x,' '))#
  x <- subset(x,nchar(x)>0)#
  x <- paste(x,collapse=' ')#
}#
#
# Each term is assigned to a topic, so this will tally for a document & assign the most frequent as membership#
docAssignment<-function(x){#
  x <- table(x)#
  x <- as.matrix(x)#
  x <- t(x)#
  x <-max.col(x)#
}#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
# Stopwords#
stops <- c(stopwords('SMART'), 'pakistan', 'gmt', 'pm')#
#
# Data articles from ~2016-04-04#
text <- readRDS("Guardian_text.rds")
text$body[1]
# String clean up #
text$body <- iconv(text$body, "latin1", "ASCII", sub="")#
text$body <- gsub('http\\S+\\s*', '', text$body ) #rm URLs; qdap has rm_url same outcome. #
text$body <- bracketX(text$body , bracket="all") #rm strings in between parenteses, and other brackets#
text$body <- replace_abbreviation(text$body ) # replaces a.m. to AM etc#
text$body[1]#
#
# Instead of DTM/TDM, just clean the vector w/old functions#
txt <- VCorpus(VectorSource(text$body))#
txt <- cleanCorpus(txt, stops)#
#
# Extract the clean text#
txt <- unlist(pblapply(txt, content))#
#
# Remove any blanks, happens sometimes w/tweets bc small length & stopwords#
txt <- pblapply(txt, blankRemoval)#
#
# Lexicalize#
txtLex <- lexicalize(txt)
# Examine the vocab or key and value pairing between key ()#
head(txtLex$vocab) # rememnber #6#
length(txtLex$vocab) #8k+ unique words among all articles, each #
head(txtLex$documents[[1]]) #look at [,22]#
head(txtLex$documents[[20]])
# Corpus stats#
txtWordCount  <- word.counts(txtLex$documents, txtLex$vocab)#
txtDocLength  <- document.lengths(txtLex$documents)
# #
k       <- 5 # number of topics#
numIter <- 25 # number of reviews, it performs random word sampling each time#
alpha   <- 0.02 #see above #
eta     <- 0.02 #see above#
set.seed(1234) #
fit <- lda.collapsed.gibbs.sampler(documents      = txtLex$documents, #
                                   K              = k, #
                                   vocab          = txtLex$vocab, #
                                   num.iterations = numIter, #
                                   alpha          = alpha, #
                                   eta            = eta, #
                                   initial        = NULL, #
                                   burnin         = 0,#
                                   compute.log.likelihood = TRUE)#
#
# Prototypical Document#
top.topic.documents(fit$document_sums,2) #top 2 docs (r
fit$document_sums #topics by articles#
head(t(fit$topics))
# LDAvis params#
# normalize the article probabilites to each topic#
theta <- t(pbapply(fit$document_sums + alpha, 2, function(x) x/sum(x))) # topic probabilities within a doc will sum to 1
# normalize each topic word's impact to the topic#
phi  <- t(pbapply(fit$topics + eta, 1, function(x) x/sum(x)))#
#
ldaJSON <- createJSON(phi = phi,#
                      theta = theta, #
                      doc.length = txtDocLength, #
                      vocab = txtLex$vocab, #
                      term.frequency = as.vector(txtWordCount))#
#
serVis(ldaJSON)
install.packages('servr')
# normalize each topic word's impact to the topic#
phi  <- t(pbapply(fit$topics + eta, 1, function(x) x/sum(x)))#
#
ldaJSON <- createJSON(phi = phi,#
                      theta = theta, #
                      doc.length = txtDocLength, #
                      vocab = txtLex$vocab, #
                      term.frequency = as.vector(txtWordCount))#
#
serVis(ldaJSON)
top.topic.words(fit$topics, 10, by.score=TRUE)
# Name Topics#
topFive <- top.topic.words(fit$topics, 5, by.score=TRUE)#
topFive <- apply(topFive,2,paste, collapse='_') #collapse each of the single topics word into a single "name"#
#
# Topic fit for first 10 words of 2nd doc#
fit$assignments[[2]][1:10]
# Tally the topic assignment for the second doc, which topic should we assign it to?#
table(fit$assignments[[2]])#
#
# What topic is article 1 assigned to?#
singleArticle <- docAssignment(fit$assignments[[1]])#
#
# Get numeric assignments for all docs#
topicAssignments <- unlist(pblapply(fit$assignments,#
                                    docAssignment))#
topicAssignments
# Recode to the top words for the topics; instead of topic "1", "2" use the top words identified earlier#
length(topicAssignments)#
assignments <- recode(topicAssignments, topFive[1], topFive[2], #
                      topFive[3],topFive[4],topFive[5])#
#
# Polarity calc to add to visual#
txtPolarity <- polarity(txt)[[1]][3]
txtPolarity <- readRDS('txtPolarity.rds')
# Final Organization#
allTree <- data.frame(topic    = assignments, #
                      polarity = txtPolarity,#
                      length   = txtDocLength)#
head(allTree)#
#
set.seed(1237)#
tmap <- treemap(allTree,#
                index   = c("topic","length"),#
                vSize   = "length",#
                vColor  = "polarity",#
                type    ="value", #
                title   = "Guardan Articles mentioning Pakistan",#
                palette = c("red","white","green"))#
#
# echart4R limited color scaling#
tmap$tm %>% #
  e_charts() %>% #
  e_treemap(parent = topic,#
            child  = length,#
            value  = vSize) %>%#
  e_color(c("red", "blue",#
            "#d3d3d3", '#bada55', 'pink')) %>%#
  e_title("Guardan Articles mentioning Pakistan")
head(head(allTree))
tmap
tmap$tm %>% #
  e_charts() %>% #
  e_treemap(topic,#
            length,#
            vSize) %>%#
  e_color(c(tmap$tm$color)) %>%#
  e_title("Guardan Articles mentioning Pakistan")
tmap$tm
tmap$tm %>% #
  e_charts() %>% #
  e_treemap(parent = topic,#
            child  = length,#
            value  = vSize)
x <- tmap$tm#
x %>% #
  e_charts() %>% #
  e_treemap(parent = topic,#
            child  = length,#
            value  = vSize)
names(x)
class(x)
getwd()
saveRDS(txtPolarity, 'txtPolarity.rds')
txtPolarity
getwd()
rm(list = ls())
# Set the working directory#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data")#
#
# Libs#
library(tm)#
library(lsa)#
#
# Bring in our supporting functions#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')#
#
# Options & Functions#
options(stringsAsFactors = FALSE, scipen = 999)#
Sys.setlocale('LC_ALL','C')#
#
# Create custom stop words#
stops <- c(stopwords('SMART'), 'car', 'electronic')
install.packages('lsa')
# Set the working directory#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data")#
#
# Libs#
library(tm)#
library(lsa)#
#
# Bring in our supporting functions#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')#
#
# Options & Functions#
options(stringsAsFactors = FALSE, scipen = 999)#
Sys.setlocale('LC_ALL','C')#
#
# Create custom stop words#
stops <- c(stopwords('SMART'), 'car', 'electronic')
# Bring in some data#
carCorp <- VCorpus(DirSource("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data/AutoAndElectronics/rec.autos"))#
electronicCorp <- VCorpus(DirSource("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data/AutoAndElectronics/rec.autos"))
# Clean each one#
carCorp        <- cleanCorpus(carCorp, stops)#
electronicCorp <- cleanCorpus(electronicCorp, stops)#
#
# Combine#
allPosts <-  c(carCorp,electronicCorp)#
rm(carCorp)#
rm(electronicCorp)#
gc()
# Construct the Target#
yTarget <- c(rep(1,1000), rep(0,1000)) #1= about cars, 0 = electronics#
#
# Make TDM; lsa docs save DTM w/"documents in colums, terms in rows and occurrence frequencies in the cells."!#
allTDM <- TermDocumentMatrix(allPosts, #
                             control = list(weighting = weightTfIdf))#
allTDM
lsaTDM <- lsa(allTDM, 20)
getwd()
saveRDS(lsaTDM, 'lsaTDM_tfidf.rds')
lsaTDM <- readRDS('lsaTDM_tfidf.rds')
reducedTopicSpace <- as.data.frame(as.matrix(lsaTDM$dk))
maxTopic  <- max.col(reducedTopicSpace)
lsaResult <- data.frame(rownames(reducedTopicSpace), maxTopic)
head(lsaResult)
head(reducedTopicSpace)
table(lsaResult$maxTopic)
getwd()
# Wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data")#
#
# Libs#
library(tm)#
library(clue)#
library(cluster)#
library(fst)#
library(wordcloud)#
#
# This is an orphaned lib which gives us plotcluster:#
#https://www.rdocumentation.org/packages/fpc/versions/2.1-11.2#
#library(fpc)#
#
# Bring in our supporting functions#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_plotCluster.R')#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
# Stopwords#
stops  <- c(stopwords('SMART'), 'work')#
#
# Read & Preprocess#
txtMat <- cleanMatrix(pth = 'basicResumes.csv', #
                      columnName  = 'text', # text column name#
                      collapse = F, #
                      customStopwords = stops, #
                      type = 'DTM', #
                      wgt = 'weightTfIdf')
txtMat    <- scale(txtMat) #subtract mean  & divide by stDev#
txtKMeans <- kmeans(txtMat, 3)#
txtKMeans$size#
barplot(txtKMeans$size, main = 'k-means')
plotcluster(cmdscale(dist(txtMat)),txtKMeans$cluster)
dissimilarityMat <- dist(txtMat)#
silPlot          <- silhouette(txtKMeans$cluster, dissimilarityMat)#
plot(silPlot, col=1:max(txtKMeans$cluster), border=NA)
#calculate indices of closest document to each centroid#
idx <- vector()#
for (i in 1:max(txtKMeans$cluster)){#
  # Calculate the absolute distance between doc & cluster center#
  absDist <- abs(txtMat[which(txtKMeans$cluster==i),] -  txtKMeans$centers[i,])#
  # Check for single doc clusters#
  if(is.null(nrow(absDist))==F){#
    absDist <- rowSums(absDist)#
    minDist <- subset(absDist, absDist==min(absDist))#
  } else {#
    minDist <- txtKMeans$cluster[txtKMeans$cluster==i]#
  }#
  idx[i] <- as.numeric(names(minDist))#
}#
#
# Notification of closest doc to centroid#
cat(paste('cluster',1:max(txtKMeans$cluster),': centroid doc is ', idx,'\n'))
# Wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data")#
#
# Libs#
library(kmed)#
library(tm)#
library(clue)#
library(cluster)#
library(wordcloud)#
#
# Bring in our supporting functions#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_plotCluster.R')#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
# Stopwords#
stops <- c(stopwords('SMART'), 'work')#
#
# Read & Preprocess#
txtMat <- cleanMatrix(pth = 'basicResumes.csv', #
                      columnName  = 'text', # clue answer text #
                      collapse = F, #
                      customStopwords = stops, #
                      type = 'DTM', #
                      wgt = 'weightTfIdf') #weightTf or weightTfIdf#
#
# Remove empty docs w/TF-Idf#
txtMat <- subset(txtMat, rowSums(txtMat) > 0)
# Use a manhattan distance matrix; default for kmed#
manhattanDist <- distNumeric(txtMat, txtMat, method = "mrw")
# Calculate the k-mediod#
txtKMeds <- fastkmed(manhattanDist, ncluster = 5, iterate = 5)
# Number of docs per cluster#
table(txtKMeds$cluster)
# Visualize separation#
plotcluster(manhattanDist, txtKMeds$cluster, pch = txtKMeds$cluster)
# Silhouette#
silPlot          <- silhouette(txtKMeds$cluster, manhattanDist)#
plot(silPlot, col=1:max(txtKMeds$cluster), border=NA)
txtKMeds$medoid
getwd()
# Wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data")#
#
# Libs#
library(skmeans)#
library(tm)#
library(clue)#
library(cluster)#
library(wordcloud)#
#
# This is an orphaned lib which gives us plotcluster:#
#https://www.rdocumentation.org/packages/fpc/versions/2.1-11.2#
#library(fpc)#
#
# Bring in our supporting functions#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_plotCluster.R')#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
# Stopwords#
stops  <- c(stopwords('SMART'), 'jeopardy')#
#
# Read & Preprocess#
txtMat <- cleanMatrix(pth = 'jeopardyArchive_1000.fst', #
                      columnName  = 'clue', # clue answer text #
                      collapse = F, #
                      customStopwords = stops, #
                      type = 'DTM', #
                      wgt = 'weightTfIdf') #weightTf or weightTfIdf#
#
# Remove empty docs w/TF-Idf#
txtMat <- subset(txtMat, rowSums(txtMat) > 0)#
#
# Apply Spherical K-Means#
txtSKMeans <- skmeans(txtMat, # data#
                      3, #clusters#
                      m = 1, #"fuzziness of cluster" 1 = hard partition, >1 increases "softness"#
                      control = list(nruns = 5, verbose = T))#
barplot(table(txtSKMeans$cluster), main = 'spherical k-means')
plotcluster(cmdscale(dist(txtMat)),txtSKMeans$cluster)
plotcluster(cmdscale(dist(txtMat)),txtSKMeans$cluster)
sk <- silhouette(txtSKMeans)#
plot(sk, col=1:3, border=NA)
# ID protypical terms#
protoTypical           <- t(cl_prototypes(txtSKMeans))#
colnames(protoTypical) <- paste0('cluster_',1:ncol(protoTypical))#
head(protoTypical)#
comparison.cloud(protoTypical)
clusterWC <- list()#
for (i in 1:ncol(protoTypical)){#
  jsWC <- data.frame(term = rownames(protoTypical),#
                     freq = protoTypical[,i])#
  jsWC <- jsWC[order(jsWC$freq, decreasing = T),]#
  clusterWC[[i]] <- wordcloud2::wordcloud2(head(jsWC[1:200,]))#
  print(paste('plotting',i))#
}#
#
clusterWC[[1]]#
clusterWC[[2]]#
clusterWC[[3]]
# Examine a portion of the most prototypical terms per cluster; usually presidents, geography & sometimes authors/playwrites#
nTerms <- 5#
(clustA <- sort(protoTypical[,1], decreasing = T)[1:nTerms])#
(clustB <- sort(protoTypical[,2], decreasing = T)[1:nTerms]) #
(clustC <- sort(protoTypical[,3], decreasing = T)[1:nTerms])
getwd()
# wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data")#
#
# options#
options(scipen = 999, stringsAsFactors = F)#
#
# Libs#
library(skmeans)#
library(tidytext)#
library(tm)#
library(clue)#
library(cluster)#
library(wordcloud)#
library(lexicon)#
library(plyr)#
library(dplyr)#
library(radarchart)
# Custom Functions#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_supportingFunctions.R')#
source('/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/Z_otherScripts/ZZZ_plotCluster.R')
# Examine Raw Text#
rawTxt <- read.csv('exampleNews.csv')#
t(rawTxt[1,])#
#
# Organize into a DF for TM#
allInfo <- data.frame(doc_id = 1:nrow(rawTxt),#
                      text   = paste0(rawTxt$title, #
                                      rawTxt$description, #
                                      rawTxt$content),#
                      source = rawTxt$id)
stops  <- c(stopwords('SMART'),'chars')
# Process#
allInfo    <- VCorpus(DataframeSource(allInfo))#
allInfo    <- cleanCorpus(allInfo, stops) #
#saveRDS(allInfo, 'allInfo.rds')#
allInfo    <- readRDS('allInfo.rds')#
allInfoDTM <-  DocumentTermMatrix(allInfo)#
allInfoDTM <- as.matrix(allInfoDTM)#
allInfoDTM <- subset(allInfoDTM, rowSums(allInfoDTM) > 0)#
dim(allInfoDTM)
set.seed(1234)#
txtSKMeans <- skmeans(allInfoDTM, #
                      4, #
                      m = 1, #
                      control = list(nruns = 5, verbose = T))
barplot(table(txtSKMeans$cluster), main = 'spherical k-means')#
plot(silhouette(txtSKMeans), col=1:2, border=NULL)
protoTypical           <- t(cl_prototypes(txtSKMeans))#
colnames(protoTypical) <- paste0('cluster_',1:ncol(protoTypical))#
head(protoTypical)
pdf(file = "/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/C_Sentiment_Unsupervised/data/news_cluster_topics.pdf", #
    width  = 6, #
    height = 6) #
comparison.cloud(protoTypical, title.size=1.1, scale=c(1,.5))#
dev.off()
#### Perform an NRC Sentiment Inner Join#
tidyCorp <- tidy(DocumentTermMatrix(allInfo))#
tidyCorp
(sourceID <- unique(meta(allInfo)))
# Process#
allInfo    <- VCorpus(DataframeSource(allInfo))#
allInfo    <- cleanCorpus(allInfo, stops)
q('no')
