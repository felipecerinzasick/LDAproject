# Libraries#
library(jsonlite)#
library(xts)#
library(zoo)#
library(dygraphs)#
library(lubridate)#
#
# Original page#
# https://projects.fivethirtyeight.com/trump-approval-ratings/#
#
# Developer Tab has two API endpoints#
historicalURL <- 'https://projects.fivethirtyeight.com/trump-approval-ratings/historical-approval.json'#
#
trumpURL      <- 'https://projects.fivethirtyeight.com/trump-approval-ratings/approval.json'#
# Get Historical#
approvalRatings <- fromJSON(historicalURL)#
head(approvalRatings)#
#
# Get Trump#
trumpApproval   <- fromJSON(trumpURL)#
head(trumpApproval)#
tail(trumpApproval)#
#
# Subset to "All polls", and not future predictions & just estimates#
trumpApproval <- subset(trumpApproval, #
                        trumpApproval$subgroup == 'All polls' & #
                          trumpApproval$future == F)#
disapprove <- ts(trumpApproval$disapprove_estimate, #
                 start = c(2017, 23), #
                 frequency = 365)#
approve    <- ts(trumpApproval$approve_estimate, start = c(2017, 23), #
                 frequency = 365)#
#
ratings <- cbind(disapprove, approve)#
ratings <- as.zoo(ratings)#
ratings <- as.xts(ratings, date_decimal(index(ratings)))#
#
dygraph(ratings, "Trump Approval") %>%#
  dySeries("approve", label = "approve", color = 'green') %>%#
  dySeries("disapprove", label = "disapprove", color = 'red') %>%#
  dyRangeSelector()
# Libraries#
library(readxl)#
#
# RCloud has small instances so clean out mem for example#
rm(list = ls())#
#
# wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/E_SyntacticParsing_DataSources/data")#
#
# File Path#
filePath <- 'exampleExcel.xlsx'#
#
# Identify sheets#
numSheets <- excel_sheets(filePath)#
numSheets#
#
# Get individual sheets#
sheetOne   <- read_excel(filePath, 1)#
sheetTwo   <- read_excel(filePath, 2)#
sheetThree <- read_excel(filePath, 3)#
#
# End
# libraries#
library(docxtractr)#
library(xml2)#
#
# wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/E_SyntacticParsing_DataSources/data")#
#
# file#
filePath <- 'exampleWord.docx'#
#
# Easiest way to get the table data#
info      <- docxtractr::read_docx(filePath)#
tableData <- docx_extract_all_tbls(info)#
tableData <- as.data.frame(tableData)#
tableData#
#
# Easiest way to extract the simple text as a single vector but requires clean up#
library(textreadr)#
info <- textreadr::read_docx(filePath)#
info#
# End
library(docxtractr)
install.packages('textreadr')
# libraries#
library(docxtractr)#
library(xml2)#
#
# wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/E_SyntacticParsing_DataSources/data")#
#
# file#
filePath <- 'exampleWord.docx'#
#
# Easiest way to get the table data#
info      <- docxtractr::read_docx(filePath)#
tableData <- docx_extract_all_tbls(info)#
tableData <- as.data.frame(tableData)#
tableData#
#
# Easiest way to extract the simple text as a single vector but requires clean up#
library(textreadr)#
info <- textreadr::read_docx(filePath)#
info#
# End
library(textcat)
# Set the working directory#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/E_SyntacticParsing_DataSources/data")#
#
# Libs#
library(textcat)#
#
# Options & Functions#
testing <- T#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
# Data#
if(testing == T){#
  unknownLanguageOne <- read.csv("tweets_Haddad_Fernando.csv", #
                                 nrows = 100)#
} else {#
  unknownLanguageOne <- read.csv("tweets_Haddad_Fernando.csv")#
}#
#
# Example languages supported#
t(t(names(TC_byte_profiles)))#
# Options for the profiles#
attr(TC_char_profiles, "options")[c("n", "size", "reduce", "useBytes")]#
#
# Categorize the language#
txtLanguage <- textcat(unknownLanguageOne$text)#
#
# Review; overall OK!#
head(txtLanguage, 10)#
#
# Problematic texts; perhaps cleaning or longer passages would help#
unknownLanguageOne$text[3]#
unknownLanguageOne$text[4]#
unknownLanguageOne$text[9]#
#
# Most frequent#
table(txtLanguage)#
#
# End
library(tesseract)
install.packages('tesseract')
# Wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/E_SyntacticParsing_DataSources/data")#
#
# libs#
library(tesseract)#
#
# Image file location; can be a folder#
img <- 'https://prestonjg.files.wordpress.com/2015/07/ny-times-moon-front.jpg'#
#http://jeroen.github.io/images/testocr.png #ENG#
#http://a2010.kiosko.net/10/30/fr/lemonde.750.jpg #FRA#
#
# Declare the language#
#tesseract_info()$available#
#tesseract_download("fra")#
eng  <- tesseract("eng")#
#
# Perform Optical Character Reco#
text <- ocr(img, engine = eng)#
cat(text)#
#
# You can also get coordinates & confidence for each letter#
results <- ocr_data(img, engine = eng)#
results
# Library#
library(rvest)#
library(stringi)#
#
# wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/E_SyntacticParsing_DataSources/data")#
# Instructor Page#
webpage <- 'http://www.gserm.ch/stgallen/instructors/'#
#
# Get all links#
getLinks <- read_html(webpage) %>% #
  html_nodes(".instructor") %>% #
  html_attr('href')#
getLinks#
#
# Extract and clean names#
instructorNames <- gsub('https://www.gserm.ch/stgallen/instructor/',#
                        '', #
                        getLinks) #
instructorNames#
#
instructorNames <- gsub("/", "", instructorNames)#
instructorNames <- gsub("-", " ", instructorNames)#
instructorNames <- stri_trans_totitle(instructorNames)#
instructorNames#
#
# Follow links to get more bio's#
allBios <- list()#
for (i in 1:length(instructorNames)){#
  # Progress Msg#
  cat(instructorNames[i])#
  # Get the bio text#
  x <- read_html(getLinks[i]) %>% #
    html_nodes(".text") %>% html_text()#
  # Get the photo URL#
  y <- read_html(getLinks[i]) %>% #
    html_nodes(".instructor-photo") %>% html_attr("src")#
  df <- data.frame(instructorName = instructorNames[i],#
                   bio            = x,#
                   photo          = y)#
  cat(' complete\n')#
  allBios[[i]] <- df#
}#
#
# Arrange the list to a single data frame#
allBios     <- do.call(rbind, allBios)#
#
# Drop the line returns #
allBios$bio <- gsub("[\r\n\t]", "", allBios$bio)#
allBios[14,]#
#
write.csv(allBios, #
          'allBios.csv',#
          row.names = F)#
# End
# Instructor Page#
webpage <- 'http://www.gserm.ch/stgallen/instructors/'#
#
# Get all links#
getLinks <- read_html(webpage) %>% #
  html_nodes(".instructor") %>% #
  html_attr('href')#
getLinks
q('no')
