setwd("~/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")
library(tm)
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url)) #
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}
stops <- c(stopwords('english'), 'lol', 'smh')
text <- read.csv('coffee.csv', header=TRUE)
names(text)[1] <- 'doc_id'
txtCorpus <- VCorpus(DataframeSource(text))
txtCorpus <- cleanCorpus(txtCorpus, stops)
txtCorpus[[4]]#
meta(txtCorpus[4])#
content(txtCorpus[[4]])
df <- data.frame(text = unlist(sapply(txtCorpus, `[`, "content")),#
                 stringsAsFactors=F)
text$text[4]#
df[4,]
txtDtm  <- DocumentTermMatrix(txtCorpus)#
txtTdm  <- TermDocumentMatrix(txtCorpus)#
txtDtmM <- as.matrix(txtDtm)#
txtTdmM <- as.matrix(txtTdm)
txtDtmM[610:611,491:493]#
txtTdmM[491:493,610:611]
txtDtmM[610:611,4]
txtDtmM[610:611,4:5]
txtDtmM[610:611,491:493]#
txtTdmM[491:493,610:611]
txtDtmM[4:6,491:493]
txtDtmM[610:611,491:493]#
txtTdmM[491:493,610:611]
topTermsA <- colSums(txtDtmM)#
topTermsB <- rowSums(txtTdmM)
topTermsA <- data.frame(terms = colnames(txtDtmM), freq = topTermsA)#
topTermsB <- data.frame(terms = rownames(txtTdmM), freq = topTermsB)
rownames(topTermsA) <- NULL#
rownames(topTermsB) <- NULL
head(topTermsA)#
head(topTermsB)
exampleReOrder <- topTermsA[order(topTermsA$freq, decreasing = T),]#
head(exampleReOrder)
idx <- which.max(topTermsA$freq)#
topTermsA[idx, ]
setwd("~/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")
# Libs#
library(tm)#
library(qdap)#
library(ggplot2)#
library(ggthemes)#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}#
#
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url))#
  #corpus <- tm_map(corpus, content_transformer(replace_contraction)) #
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}#
#
# Create custom stop words#
stops <- c(stopwords('SMART'), 'amp', 'britishairways', 'british',#
                     'flight', 'flights', 'airways')#
#
# Read in Data, clean & organize#
text      <- read.csv('BritishAirways.csv')#
txtCorpus <- VCorpus(VectorSource(text$text))#
txtCorpus <- cleanCorpus(txtCorpus, stops)#
tweetTDM  <- TermDocumentMatrix(txtCorpus)#
tweetTDMm <- as.matrix(tweetTDM)
tweetSums <- rowSums(tweetTDMm)#
tweetFreq <- data.frame(word=names(tweetSums),frequency=tweetSums)#
#
# Review a section#
tweetFreq[50:55,]#
#
# Remove the row attributes meta family#
rownames(tweetFreq) <- NULL#
tweetFreq[50:55,]#
#
# Simple barplot; values greater than 15#
topWords      <- subset(tweetFreq, tweetFreq$frequency >= 15) #
topWords      <- topWords[order(topWords$frequency, decreasing=F),]#
#
# Chg to factor for ggplot#
topWords$word <- factor(topWords$word, #
                        levels=unique(as.character(topWords$word))) #
#
ggplot(topWords, aes(x=word, y=frequency)) + #
  geom_bar(stat="identity", fill='darkred') + #
  coord_flip()+ theme_gdocs() +#
  geom_text(aes(label=frequency), colour="white",hjust=1.25, size=3.0)
ggplot(topWords, aes(x=word, y=frequency)) + #
  geom_bar(stat="identity", fill='darkred') + #
  coord_flip()+ theme_gdocs() +#
  geom_text(aes(label=frequency), colour="white",hjust=1.25, size=3.0)
plot(freq_terms(text$text, top=35, at.least=2, stopwords = stops))
associations <- findAssocs(tweetTDM, 'brewdog', 0.30)#
associations#
#
# Organize the word associations#
assocDF <- data.frame(terms=names(associations[[1]]),#
                       value=unlist(associations))#
assocDF$terms <- factor(assocDF$terms, levels=assocDF$terms)#
rownames(assocDF) <- NULL#
assocDF#
#
# Make a dot plot#
ggplot(assocDF, aes(y=terms)) +#
  geom_point(aes(x=value), data=assocDF, col='#c00c00') +#
  theme_gdocs() + #
  geom_text(aes(x=value,label=value), colour="red",hjust="inward", vjust ="inward" , size=3)
setwd("~/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")
# Libs#
library(tm)#
library(qdap)#
library(ggplot2)#
library(ggthemes)
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}#
#
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url))#
  #corpus <- tm_map(corpus, content_transformer(replace_contraction)) #
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}#
#
# Create custom stop words#
stops <- c(stopwords('SMART'), 'amp', 'britishairways', 'british',#
           'flight', 'flights', 'airways')#
#
# Read in Data, clean & organize#
text      <- read.csv('BritishAirways.csv')#
txtCorpus <- VCorpus(VectorSource(text$text))#
txtCorpus <- cleanCorpus(txtCorpus, stops)#
tweetTDM  <- TermDocumentMatrix(txtCorpus)
reducedTDM <- removeSparseTerms(tweetTDM, sparse=0.985) #shoot for ~50 terms; 1.5% of cells in row have a value  #
reducedTDM
reducedTDM <- as.data.frame(as.matrix(reducedTDM))
hc <- hclust(dist(reducedTDM))#
plot(hc,yaxt='n')
networkStops <- c(stops, 'britishairways', 'british', 'airways', 'rt')
assocText <- rm_url(text$text)
word_associate(assocText, #
               match.string = 'brewdog', #
               stopwords = networkStops,#
               network.plot = T,#
               cloud.colors = c('black','darkred'))
word_associate(assocText, #
               match.string = 'brewdog', #
               stopwords = networkStops,#
               wordcloud = T,#
               cloud.colors = c('black','darkred'))
setwd("~/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")
# Libs#
library(tm)#
library(qdap) # Comment out if you have qdap problems#
library(wordcloud)#
library(RColorBrewer)#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}#
#
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url))#
  #corpus <- tm_map(corpus, content_transformer(replace_contraction)) #
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}#
#
# Create custom stop words#
stops <- c(stopwords('english'), 'lol', 'amp', 'and', 'chardonnay')#
#
# Bigram token maker#
bigramTokens <-function(x){#
  unlist(lapply(NLP::ngrams(words(x), 2), paste, collapse = " "), #
         use.names = FALSE)#
}
# Data#
text <- read.csv('chardonnay.csv', header=TRUE)
# As of tm version 0.7-3 tabular was deprecated#
names(text)[1] <-'doc_id' #
#
# Make a volatile corpus#
txtCorpus <- VCorpus(DataframeSource(text))#
#
# Preprocess the corpus#
txtCorpus <- cleanCorpus(txtCorpus, stops)#
#
# Make bi-gram TDM according to the tokenize control & convert it to matrix#
wineTDM  <- TermDocumentMatrix(txtCorpus, #
                               control=list(tokenize=bigramTokens))#
wineTDMm <- as.matrix(wineTDM)#
#
# See a bi-gram#
exampleTweet <- grep('wine country', rownames(wineTDMm))#
wineTDMm[(exampleTweet-2):(exampleTweet),870:871]
# Get Row Sums & organize#
wineTDMv <- sort(rowSums(wineTDMm), decreasing = TRUE)#
wineDF   <- data.frame(word = names(wineTDMv), freq = wineTDMv)
display.brewer.all()
# Choose a color & drop light ones#
pal <- brewer.pal(8, "Purples")#
pal <- pal[-(1:2)]#
#
# Make simple word cloud#
# Reminder to expand device pane#
set.seed(1234)#
wordcloud(wineDF$word,#
          wineDF$freq,#
          max.words    = 50,#
          random.order = FALSE,#
          colors       = pal,#
          scale        = c(2,1))
wordcloud(wineDF$word,#
          wineDF$freq,#
          max.words    = 50,#
          random.order = FALSE,#
          colors       = pal,#
          scale        = c(2,1))
getwd()
/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data
# Libs#
library(tm)#
library(qdap)#
library(wordcloud2)#
library(RColorBrewer)#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}#
#
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url))#
  #corpus <- tm_map(corpus, content_transformer(replace_contraction)) #
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}#
#
# Create custom stop words#
stops <- c(stopwords('english'), 'lol', 'amp', 'chardonnay')#
#
# Bigram token maker#
bigramTokens <-function(x){#
  unlist(lapply(NLP::ngrams(words(x), 2), paste, collapse = " "), #
         use.names = FALSE)#
}#
#
# Data#
text <- read.csv('chardonnay.csv', header=TRUE)#
#
# As of tm version 0.7-3 tabular was deprecated#
names(text)[1]<-'doc_id' #
#
# Make a volatile corpus#
txtCorpus <- VCorpus(DataframeSource(text))#
#
# Preprocess the corpus#
txtCorpus <- cleanCorpus(txtCorpus, stops)#
#
# Make bi-gram TDM according to the tokenize control & convert it to matrix#
wineTDM  <- TermDocumentMatrix(txtCorpus, #
                               control=list(tokenize=bigramTokens))#
wineTDMm <- as.matrix(wineTDM)#
#
# See a bi-gram#
exampleTweet <- grep('wine country', rownames(wineTDMm))#
wineTDMm[(exampleTweet-2):(exampleTweet),870:871]#
#
# Get Row Sums & organize#
wineTDMv <- sort(rowSums(wineTDMm), decreasing = TRUE)#
wineDF   <- data.frame(word = names(wineTDMv), freq = wineTDMv)#
#
# Regular dynamic WC, click the pop-out in the viewer#
wordcloud2(data = wineDF[1:50,])
# Choose a color & drop light ones#
pal <- brewer.pal(8, "Dark2")#
wordcloud2(wineDF[1:50,], #
           color = pal, #
           backgroundColor = "lightgrey")
# Some built in shapes need to click "show in new window"#
# 'circle', 'cardioid', 'diamond', 'triangle-forward', 'triangle', 'pentagon', & 'star'#
wordcloud2(wineDF[1:50,],#
           shape = "cardioid",#
           color = "blue",#
           backgroundColor = "pink")
getwd()
rm(list = ls())
# Set the working directory#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")#
#
# Libs#
library(tm)#
library(qdap)#
library(wordcloud)#
library(RColorBrewer)#
library(pbapply)#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}#
#
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url))#
  corpus <- tm_map(corpus, content_transformer(replace_contraction)) #
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}#
#
# Create custom stop words#
stops <- c(stopwords('english'), 'lol', 'amp', 'chardonnay', 'coffee')#
#
# Read in multiple files as individuals#
txtFiles <- list.files(pattern = 'chardonnay|coffee')#
#
for (i in 1:length(txtFiles)){#
  assign(txtFiles[i], read.csv(txtFiles[i]))#
  cat(paste('read completed:',txtFiles[i],'\n'))#
}
# Vector Corpus; omit the meta data#
chardonnay <- VCorpus(VectorSource(chardonnay.csv$text))#
coffee     <- VCorpus(VectorSource(coffee.csv$text))#
#
# Clean up the data#
chardonnay <- cleanCorpus(chardonnay, stops)#
coffee     <- cleanCorpus(coffee, stops)#
#
# Another way to extract the cleaned text #
chardonnay <- unlist(pblapply(chardonnay, content))#
coffee     <- unlist(pblapply(coffee, content))
length(chardonnay)
# Instead of 1000 individual documents, collapse each into a single "subject" ie a single document#
chardonnay <- paste(chardonnay, collapse = ' ')#
coffee     <- paste(coffee, collapse = ' ')
# FYI pt2#
length(chardonnay)#
#
# Combine the subject documents into a corpus of *2* documents#
allDrinks <- c(chardonnay, coffee)#
allDrinks <- VCorpus((VectorSource(allDrinks)))
allDrinks
# Make TDM#
drinkTDM  <- TermDocumentMatrix(allDrinks)#
drinkTDMm <- as.matrix(drinkTDM)
# Make sure order is correct!#
colnames(drinkTDMm) <- c('chardonnay', 'coffee')
head(drinkTDMm)
commonality.cloud(drinkTDMm, #
                  max.words=150, #
                  random.order=FALSE,#
                  colors='blue',#
                  scale=c(3.5,0.25))
getwd()
# Set the working directory#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")#
#
# Options#
options(scipen = 999)#
#
# Libs#
library(tm)#
library(qdap)#
library(wordcloud)#
library(RColorBrewer)#
library(pbapply)#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}#
#
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url))#
  #corpus <- tm_map(corpus, content_transformer(replace_contraction)) #
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}#
#
# Create custom stop words#
stops <- c(stopwords('english'), 'lol', 'amp', 'chardonnay', 'beer')#
#
# Read in multiple files as individuals#
txtFiles <- list.files(pattern = 'beer|chardonnay')#
#
for (i in 1:length(txtFiles)){#
  assign(txtFiles[i], read.csv(txtFiles[i]))#
  cat(paste('read completed:',txtFiles[i],'\n'))#
}#
#
# Vector Corpus; omit the meta data#
beer       <- VCorpus(VectorSource(beer.csv$text))#
chardonnay <- VCorpus(VectorSource(chardonnay.csv$text))#
#
# Clean up the data#
beer       <- cleanCorpus(beer, stops)#
chardonnay <- cleanCorpus(chardonnay, stops)
# Another way to extract the cleaned text #
beer       <- unlist(pblapply(beer, content))#
chardonnay <- unlist(pblapply(chardonnay, content))#
#
# FYI#
length(beer)#
#
# Instead of 1000 individual documents, collapse each into a single "subject" ie a single document#
beer       <- paste(beer, collapse = ' ')#
chardonnay <- paste(chardonnay, collapse = ' ')#
#
# FYI pt2#
length(beer)#
head(beer)#
#
# Combine the subject documents into a corpus of *2* documents#
allDrinks <- c(beer, chardonnay)#
allDrinks <- VCorpus((VectorSource(allDrinks)))
# Make TDM with a different control parameter#
# Tokenization `control=list(tokenize=bigramTokens)`#
# You can have more than 1 ie `control=list(tokenize=bigramTokens, weighting = weightTfIdf)`#
ctrl      <- list(weighting = weightTfIdf)#
drinkTDM  <- TermDocumentMatrix(allDrinks, control = ctrl)#
drinkTDMm <- as.matrix(drinkTDM)#
#
# Make sure order is the same as the c(objA, objB) on line ~81#
colnames(drinkTDMm) <- c('beer', 'chardonnay')
# Examine#
head(drinkTDMm)#
#
# Make comparison cloud#
comparison.cloud(drinkTDMm, #
                 max.words=75, #
                 random.order=FALSE,#
                 title.size=0.5,#
                 colors=brewer.pal(ncol(drinkTDMm),"Dark2"),#
                 scale=c(3,0.1))
getwd()
# Wd#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")#
#
# Libs#
library(tm)#
library(qdap) # Comment out if you have qdap problems#
library(wordcloud)#
library(RColorBrewer)#
#
# this is 130K rows, so you can read in a small amount if you want#
n <- 1000#
#
# bring in the data#
wineReviews <- read.csv(unz("wineReviewData.zip", #
                            "winemag-data-130k-v2.csv"), nrows=n)#
#
# Explore it so we know what is it#
names(wineReviews)#
head(wineReviews)
table(wineReviews$variety)
# Or plot the country#
barplot(table(wineReviews$country))
summary(wineReviews$points)#
summary(wineReviews$price)
plot(wineReviews$points,wineReviews$price)
wineDescriptions <- wineReviews$description
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}#
#
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url))#
  #corpus <- tm_map(corpus, content_transformer(replace_contraction)) #
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}
stops <- c(stopwords('english'), 'wine')#
#
# Make a volatile corpus#
txtCorpus <- VCorpus(VectorSource(wineDescriptions))
# Clean the corpus#
txtCorpus <- cleanCorpus(txtCorpus, stops)#
#
# Make bi-gram TDM according to the tokenize control & convert it to matrix#
wineTDM  <- TermDocumentMatrix(txtCorpus)#
wineTDMm <- as.matrix(wineTDM)#
#
# How many unique words?#
dim(wineTDMm)
# Get Row Sums & organize#
wineTDMv <- sort(rowSums(wineTDMm), decreasing = TRUE)#
wineDF   <- data.frame(word = names(wineTDMv), freq = wineTDMv)
# Let's make our wordcloud#
wordcloud(wineDF$word,#
          wineDF$freq,#
          max.words    = 100,#
          random.order = F,#
          colors       = c('grey', 'goldenrod', 'tomato'),#
          scale        = c(2,1))
getwd()
rm(list = ls())
# Set the working directory#
setwd("/Users/edwardkwartler/Desktop/GSERM_Text_Remote_admin/lessons/B_Basic_Visuals/data")#
#
# Libs#
library(tm)#
library(wordcloud)#
#
# Options & Functions#
options(stringsAsFactors = FALSE)#
Sys.setlocale('LC_ALL','C')#
#
tryTolower <- function(x){#
  y = NA#
  try_error = tryCatch(tolower(x), error = function(e) e)#
  if (!inherits(try_error, 'error'))#
    y = tolower(x)#
  return(y)#
}#
#
cleanCorpus<-function(corpus, customStopwords){#
  corpus <- tm_map(corpus, content_transformer(qdapRegex::rm_url)) #
  corpus <- tm_map(corpus, removePunctuation)#
  corpus <- tm_map(corpus, stripWhitespace)#
  corpus <- tm_map(corpus, removeNumbers)#
  corpus <- tm_map(corpus, content_transformer(tryTolower))#
  corpus <- tm_map(corpus, removeWords, customStopwords)#
  return(corpus)#
}#
#
bigramTokens <-function(x){#
  unlist(lapply(NLP::ngrams(words(x), 2), paste, collapse = " "), #
         use.names = FALSE)#
}#
# Create custom stop words#
stops <- c(stopwords('english'), 'loan', 'debt')#
#
# Data#
df <- read.csv('20K_sampleLoans.csv')#
df$purpose <- gsub('_', ' ', df$purpose)#
#
# smaller for tm work#
txtOutcome <- data.frame(doc_id = seq_along(df$purpose),#
                         text   = df$purpose,#
                         y      = df$y)#
#
# Make a volatile corpus#
txtCorpus <- VCorpus(DataframeSource(txtOutcome))#
#
# Preprocess the corpus#
txtCorpus <- cleanCorpus(txtCorpus, stops)#
#
# After cleaning split#
head(meta(txtCorpus))#
goodLoans <- subset(txtCorpus, meta(txtCorpus)==1)#
badLoans  <- subset(txtCorpus, meta(txtCorpus)==0)#
#
# Extract the clean txt & collapse #
goodLoans <- unlist(lapply(goodLoans, content))#
goodLoans <- paste(goodLoans, collapse = ' ')#
#
badLoans <- unlist(lapply(badLoans, content))#
badLoans <- paste(badLoans, collapse = ' ')#
#
# Combine & create corpus#
bothOutcomes <- c(goodLoans, badLoans)#
bothOutcomes <- VCorpus(VectorSource(bothOutcomes))#
#
# Make TDM#
bothTDM <- TermDocumentMatrix(bothOutcomes)#, #
#control = list(tokenize  = bigramTokens))#
bothTDM <- as.matrix(bothTDM)#
#
colnames(bothTDM) <- c('good', 'bad')#
#
comparison.cloud(bothTDM, #
                 max.words    = 15, #
                 random.order = FALSE,#
                 title.size   = 1.75,#
                 colors       = c('#bada55', 'blue'))#,scale=c(3,1))
table(df$purpose, df$y)
rm(list = ls())
